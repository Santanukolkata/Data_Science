{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The probabilistic language modeling\n",
    "\n",
    "The probability of a sentence S (as a sequence of words $w_i$ is : $P(S) = P(w_1,w_2, w_3,\\ldots,w_n)$\n",
    "\n",
    "the conditional probability of $w_4$ depending on all previous words. \n",
    "\n",
    "For a 4 word sentence this conditional probability is:\n",
    "\n",
    "$$ P(S)=P(w_1, w_2, w_3, w_4) \\equiv P(w_4 |w_1, w_2, w3)$$\n",
    "\n",
    "Probability Chain Rule:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A\\cap B)}{P(B)} \\implies P(A\\cap B)=P(A|B)P(B)$$\n",
    "\n",
    "so in general for a token sequence we get:\n",
    "\n",
    "$$P(S)=P(w_1,\\ldots,w_n) = P(w_1)P(w_2|w_1) P(w_3)P(w_1,w_2)\\ldots P(w_n|w_1,\\ldots w_{n-1}) ={\\displaystyle \\prod_{i} P(w_i|w_1,\\ldots w_{i-1})} $$\n",
    "\n",
    "To estimate each probability a straightforward solution could be to use simple counting.\n",
    "\n",
    "$$ P(w_5|w_1,w_2,w_3,w_4)= \\frac{count(w_1,w_2,w_3,w_4,w_5)}{count(w_1,w_2,w_3,w_4)}$$\n",
    "\n",
    "but this gives us to many possible sequences to ever estimate. Imagine how much data (occurrences of each sentence) we would have to get to make this counts meaningful.\n",
    "\n",
    "To cope with this issue we can simplify by applying the __Markov Assumption__, which states that it is enough to pick only one, or a couple of previous words as a prefix:\n",
    "\n",
    "$$P(w_1,\\ldots,w_n) \\approx {\\displaystyle \\prod_{i} P(w_i|w_{i-k},\\ldots P(w_{i-1}))} $$\n",
    "\n",
    "where k is the number of previous tokens (prefix size) that we consider.\n",
    "\n",
    "## N-grams\n",
    "\n",
    "An N-gram is a contiguous (order matters) sequence of items, which in this case is the words in text. The n-grams depends on the size of the prefix. The simplest case is the __Unigram mode__.\n",
    "\n",
    "#### (Uni-) 1-gram model \n",
    "\n",
    "The simplest case of __Markov assumption__ is case when the size of prefix is one.\n",
    "\n",
    "$$P(w_1,\\ldots,w_n) \\approx {\\displaystyle \\prod_{i} P(w_i)}$$\n",
    "\n",
    "This will provide us with grammar that only consider one word. As a result it produces a set of unrelated words. It actually would generate sentences with random word order.\n",
    "\n",
    "#### Bigram \n",
    "However, if we consider a 2-word (tandem) bigrams correlations where we condition each word on previous one we get some sens of meaning between couples of words.\n",
    "\n",
    "$$ P(w_1,\\ldots,w_n) \\approx {\\displaystyle \\prod_{i} P(w_i|w_{i-1})} $$\n",
    "\n",
    "This way we get a sequence of tandems that have meaning tandem-wise. This still is not enough to face a long range dependencies from natural languages, like English. This would be difficult even in case of n-grams as there can be very long sentences with dependencies. However, a 3-gram can get us a pretty nice approximation .\n",
    "\n",
    "## Estimate n-gram probabilities\n",
    "\n",
    "Estimation can be done with Maximum Likelihood Estimate (MLE):\n",
    "\n",
    "$$ P(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)}{count(w_{i-1 })} $$\n",
    "\n",
    "For example, 5th word\n",
    "\n",
    "\n",
    "$$ P(w_5|w_{4})=\\frac{count(w_{4},w_5)}{count(w_{4 })} $$\n",
    "\n",
    "In practice, the outcome should be represented in __log__ form. \n",
    "\n",
    "There are 2 reasons for this. \n",
    "\n",
    "- Firstly, if the sentence is long and the probabilities are really small, then such product might end in arithmetic underflow. \n",
    "\n",
    "- Secondly, adding is faster - and when we use logarithm we know that: $log(a*b) = log(a)+log(b)$, thus:\n",
    "\n",
    "$$log(P(w_1,\\ldots,w_n)) \\approx {\\displaystyle \\sum_{i} log(P(w_i|w_{i-1}))}$$\n",
    "\n",
    "This is why the Language Model is stored in logarithmic values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"one fish two fish red fish blue fish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_tokenize(corpus):\n",
    "    fdist[word.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"blue\": 1,\n",
      "    \"fish\": 4,\n",
      "    \"one\": 1,\n",
      "    \"red\": 1,\n",
      "    \"two\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print (json.dumps(fdist, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 'one'),\n",
       " ('one', 'fish'),\n",
       " ('fish', 'two'),\n",
       " ('two', 'fish'),\n",
       " ('fish', 'red'),\n",
       " ('red', 'fish'),\n",
       " ('fish', 'blue'),\n",
       " ('blue', 'fish'),\n",
       " ('fish', None)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.bigrams(word_tokenize(corpus), pad_left=True, pad_right=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfdist = nltk.ConditionalFreqDist(nltk.bigrams(word_tokenize(corpus), pad_left=True, pad_right=True))\n",
    "cfdist = nltk.ConditionalFreqDist(nltk.bigrams(word_tokenize(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['one', 'fish', 'two', 'red', 'blue'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfdist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([FreqDist({'fish': 1}), FreqDist({'two': 1, 'red': 1, 'blue': 1}), FreqDist({'fish': 1}), FreqDist({'fish': 1}), FreqDist({'fish': 1})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfdist.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"blue\": {\n",
      "        \"fish\": 1\n",
      "    },\n",
      "    \"fish\": {\n",
      "        \"blue\": 1,\n",
      "        \"red\": 1,\n",
      "        \"two\": 1\n",
      "    },\n",
      "    \"one\": {\n",
      "        \"fish\": 1\n",
      "    },\n",
      "    \"red\": {\n",
      "        \"fish\": 1\n",
      "    },\n",
      "    \"two\": {\n",
      "        \"fish\": 1\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print (json.dumps(cfdist, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 'fish'),\n",
       " ('fish', 'two'),\n",
       " ('two', 'fish'),\n",
       " ('fish', 'red'),\n",
       " ('red', 'fish'),\n",
       " ('fish', 'blue'),\n",
       " ('blue', 'fish')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.bigrams(word_tokenize(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cfdist[a][b] for (a, b) in nltk.bigrams(word_tokenize(corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "fish\n",
      "two\n",
      "fish\n",
      "red\n",
      "fish\n",
      "blue\n"
     ]
    }
   ],
   "source": [
    "for (a, b) in nltk.bigrams(word_tokenize(corpus)):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 1, 3, 1, 3, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cfdist[a].N() for (a, b) in nltk.bigrams(word_tokenize(corpus))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
