{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Recognition â€“ Unlock Your Application With Your Face!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1 - Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Collecting Samples Complete\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load functions\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Save file in specified directory with unique name\n",
    "        file_name_path = './faces/user1/' + str(count) + '.jpg'\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        \n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Collecting Samples Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained sucessefully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Get the training data we previously made\n",
    "data_path = './faces/user/'\n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "# Create arrays for training data and labels\n",
    "Training_Data, Labels = [], []\n",
    "\n",
    "# Open training images in our datapath\n",
    "# Create a numpy array for training data\n",
    "for i, files in enumerate(onlyfiles):\n",
    "    image_path = data_path + onlyfiles[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    Training_Data.append(np.asarray(images, dtype=np.uint8))\n",
    "    Labels.append(i)\n",
    "\n",
    "# Create a numpy array for both training data and labels\n",
    "Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "# Initialize facial recognizer\n",
    "#model = cv2.face.LBPHFaceRecognizer_create()\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "# NOTE: For OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n",
    "\n",
    "# Let's train our model \n",
    "model.train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "print(\"Model trained sucessefully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Run Our Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img, size=0.5):\n",
    "    \n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return img, []\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))\n",
    "    return img, roi\n",
    "\n",
    "\n",
    "# Open Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    image, face = face_detector(frame)\n",
    "    \n",
    "    try:\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Pass face to prediction model\n",
    "        # \"results\" comprises of a tuple containing the label and the confidence value\n",
    "        results = model.predict(face)\n",
    "        \n",
    "        if results[1] < 500:\n",
    "            confidence = int( 100 * (1 - (results[1])/400) )\n",
    "            display_string = str(confidence) + '% Confident it is User'\n",
    "            \n",
    "        cv2.putText(image, display_string, (100, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255,120,150), 2)\n",
    "        \n",
    "        if confidence > 75:\n",
    "            cv2.putText(image, \"Unlocked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "        else:\n",
    "            cv2.putText(image, \"Locked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "\n",
    "    except:\n",
    "        cv2.putText(image, \"No Face Found\", (220, 120) , cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.putText(image, \"Locked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.imshow('Face Recognition', image )\n",
    "        pass\n",
    "        \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "imshow() missing required argument 'winname' (pos 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e7b208bf19ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: imshow() missing required argument 'winname' (pos 1)"
     ]
    }
   ],
   "source": [
    "cv2.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not found\n",
      "Face not found\n",
      "Face not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:41: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Collecting Samples Complete\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load functions\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "lst=[]\n",
    "start_time=time.time()\n",
    "elapsed_time=time.time()\n",
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        start_time = time.time()\n",
    "        face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(time.clock()), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(\"Face not found\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        lst.append(elapsed_time)\n",
    "        seconds=0\n",
    "        time.sleep(1)\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        \n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Collecting Samples Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04787874221801758,\n",
       " 0.0468747615814209,\n",
       " 0.0625004768371582,\n",
       " 0.0468752384185791,\n",
       " 1.112459659576416,\n",
       " 2.1872880458831787,\n",
       " 0.04687666893005371,\n",
       " 1.1165790557861328]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lst,bins=20,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.000063896179199\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "time.sleep(5)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<VideoCapture 000000B0944DBC10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.4.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
